#!/usr/bin/env python3
"""
PyMigBench Downloader

Downloads the entire PyMigBench dataset efficiently by downloading only the parent commits
of target commits that have exactly one parent.
"""

import os
import json
import time
import zipfile
import requests
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import yaml
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys

@dataclass
class CommitInfo:
    repo: str
    commit_sha: str
    parent_sha: Optional[str] = None
    parents_count: int = 0

class PyMigBenchDownloader:
    def __init__(self, github_token: str, output_dir: str = "repos", max_workers: int = 5):
        self.github_token = github_token
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.max_workers = max_workers
        
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"token {github_token}",
            "Accept": "application/vnd.github.v3+json"
        })
        
        # Setup logging to both file and console
        log_file = self.output_dir / "download.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"PyMigBench Downloader initialized. Log file: {log_file}")

    def get_commit_parents(self, repo: str, commit_sha: str) -> Tuple[int, Optional[str]]:
        """
        Get the number of parents and the first parent SHA for a commit.
        
        Returns:
            Tuple of (parent_count, first_parent_sha)
        """
        url = f"https://api.github.com/repos/{repo}/commits/{commit_sha}"
        
        try:
            response = self.session.get(url)
            response.raise_for_status()
            
            commit_data = response.json()
            parents = commit_data.get("parents", [])
            
            parent_count = len(parents)
            first_parent_sha = parents[0]["sha"] if parents else None
            
            return parent_count, first_parent_sha
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error fetching commit {commit_sha} from {repo}: {e}")
            return 0, None

    def download_commit_zip(self, repo: str, commit_sha: str, output_path: Path) -> bool:
        """
        Download a specific commit as a zip file from GitHub.
        
        Returns:
            True if download successful, False otherwise
        """
        url = f"https://api.github.com/repos/{repo}/zipball/{commit_sha}"
        
        try:
            response = self.session.get(url, stream=True)
            response.raise_for_status()
            
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error downloading {commit_sha} from {repo}: {e}")
            return False

    def extract_zip(self, zip_path: Path, extract_to: Path) -> bool:
        """
        Extract a zip file and clean up the zip.
        
        Returns:
            True if extraction successful, False otherwise
        """
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_to)
            
            # Remove the zip file after extraction
            zip_path.unlink()
            return True
            
        except (zipfile.BadZipFile, OSError) as e:
            self.logger.error(f"Error extracting {zip_path}: {e}")
            return False

    def process_single_commit(self, commit_info: CommitInfo) -> bool:
        """
        Process a single commit: check parents, download if valid.
        
        Returns:
            True if processed successfully, False otherwise
        """
        repo_safe = commit_info.repo.replace("/", "_")
        commit_dir = self.output_dir / f"{repo_safe}__{commit_info.commit_sha}"
        
        # Skip if already downloaded (folder exists means successful download)
        if commit_dir.exists():
            self.logger.info(f"Skipping {commit_info.repo}:{commit_info.commit_sha} (already exists at {commit_dir})")
            return True
        
        # Get parent information
        parent_count, parent_sha = self.get_commit_parents(commit_info.repo, commit_info.commit_sha)
        
        if parent_count != 1:
            self.logger.info(f"Skipping {commit_info.repo}:{commit_info.commit_sha} ({parent_count} parents)")
            return False
        
        if not parent_sha:
            self.logger.error(f"No parent SHA found for {commit_info.repo}:{commit_info.commit_sha}")
            return False
        
        # Use a temporary directory for download
        temp_dir = self.output_dir / f".temp__{repo_safe}__{commit_info.commit_sha}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        zip_path = temp_dir / f"{parent_sha}.zip"
        
        try:
            # Download parent commit
            self.logger.info(f"Downloading {commit_info.repo}:{parent_sha} (parent of {commit_info.commit_sha})")
            
            if not self.download_commit_zip(commit_info.repo, parent_sha, zip_path):
                return False
            
            # Extract the zip
            if not self.extract_zip(zip_path, temp_dir):
                return False
            
            # Save metadata
            metadata = {
                "repo": commit_info.repo,
                "target_commit": commit_info.commit_sha,
                "downloaded_commit": parent_sha,
                "download_timestamp": time.time()
            }
            
            metadata_path = temp_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Only move to final location if everything succeeded
            temp_dir.rename(commit_dir)
            
            self.logger.info(f"Successfully processed {commit_info.repo}:{commit_info.commit_sha} -> {commit_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error processing {commit_info.repo}:{commit_info.commit_sha}: {e}")
            # Clean up temp directory on failure
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            return False

    def load_pymigbench_data(self, pymigbench_path: str) -> List[CommitInfo]:
        """
        Load migration data from PyMigBench dataset.
        
        Args:
            pymigbench_path: Path to the PyMigBench migration directory
            
        Returns:
            List of CommitInfo objects
        """
        migration_dir = Path(pymigbench_path)
        commits = []
        
        if not migration_dir.exists():
            raise FileNotFoundError(f"PyMigBench migration directory not found: {migration_dir}")
        
        # Find all YAML files
        yaml_files = list(migration_dir.glob("**/*.yaml")) + list(migration_dir.glob("**/*.yml"))
        
        self.logger.info(f"Found {len(yaml_files)} YAML files in {migration_dir}")
        
        for yaml_file in yaml_files:
            try:
                with open(yaml_file, 'r') as f:
                    data = yaml.safe_load(f)
                
                # Extract commit information from the YAML structure
                # This may need adjustment based on the actual PyMigBench YAML structure
                if 'repo' in data and 'commit' in data:
                    commit_info = CommitInfo(
                        repo=data['repo'],
                        commit_sha=data['commit']
                    )
                    commits.append(commit_info)
                elif 'files' in data:
                    # Handle structure where files contain repo/commit info
                    for file_data in data['files']:
                        if 'repo' in file_data and 'commit' in file_data:
                            commit_info = CommitInfo(
                                repo=file_data['repo'],
                                commit_sha=file_data['commit']
                            )
                            commits.append(commit_info)
                
            except (yaml.YAMLError, KeyError) as e:
                self.logger.warning(f"Error processing {yaml_file}: {e}")
                continue
        
        # Remove duplicates
        unique_commits = []
        seen = set()
        for commit in commits:
            key = (commit.repo, commit.commit_sha)
            if key not in seen:
                seen.add(key)
                unique_commits.append(commit)
        
        self.logger.info(f"Found {len(unique_commits)} unique commits")
        return unique_commits

    def download_all(self, pymigbench_path: str, max_count: Optional[int] = None) -> None:
        """
        Download all valid commits from PyMigBench dataset.
        
        Args:
            pymigbench_path: Path to the PyMigBench migration directory
            max_count: Optional limit on number of commits to process (for testing)
        """
        commits = self.load_pymigbench_data(pymigbench_path)
        
        if max_count:
            commits = commits[:max_count]
            self.logger.info(f"Limited to {max_count} commits for testing")
        
        self.logger.info(f"Starting download of {len(commits)} commits using {self.max_workers} workers")
        
        successful = 0
        failed = 0
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all jobs
            future_to_commit = {
                executor.submit(self.process_single_commit, commit): commit 
                for commit in commits
            }
            
            # Process completed jobs
            for future in as_completed(future_to_commit):
                commit = future_to_commit[future]
                try:
                    success = future.result()
                    if success:
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    self.logger.error(f"Exception processing {commit.repo}:{commit.commit_sha}: {e}")
                    failed += 1
                
                # Progress update
                total_processed = successful + failed
                if total_processed % 10 == 0:
                    self.logger.info(f"Progress: {total_processed}/{len(commits)} processed "
                                   f"({successful} successful, {failed} failed)")
        
        self.logger.info(f"Download complete: {successful} successful, {failed} failed")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Download PyMigBench dataset efficiently")
    parser.add_argument("pymigbench_path", help="Path to PyMigBench migration directory")
    parser.add_argument("--output-dir", default="repos", help="Output directory for downloaded repos")
    parser.add_argument("--github-token", help="GitHub API token (or set GITHUB_TOKEN env var)")
    parser.add_argument("--max-workers", type=int, default=5, help="Number of concurrent downloads")
    parser.add_argument("--max-count", type=int, help="Limit number of commits (for testing)")
    
    args = parser.parse_args()
    
    # Get GitHub token
    github_token = args.github_token or os.getenv("GITHUB_TOKEN")
    if not github_token:
        print("Error: GitHub token required. Set GITHUB_TOKEN environment variable or use --github-token")
        sys.exit(1)
    
    # Create downloader and start
    downloader = PyMigBenchDownloader(
        github_token=github_token,
        output_dir=args.output_dir,
        max_workers=args.max_workers
    )
    
    try:
        downloader.download_all(args.pymigbench_path, args.max_count)
    except KeyboardInterrupt:
        print("\nDownload interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()